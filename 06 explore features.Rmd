---
title: "06 examine features"
author: "JungHwan Yang"
date: "March 17, 2015"
output: html_document
---

#### Read the humancoded data

```{r read data, eval = F}
###############################################################################
path <- "../R Project//Data/FinalHandcoded1300.csv"
d <- read.csv (path)
###############################################################################
```

#### Divide data into training and test set using caret() package

```{r create training/test set}
###############################################################################
library(caret)
inTrain <- createDataPartition(y = d$User_Ideology, p = .75, list = F)
training <- d[inTrain, ]
testing <- d[-inTrain, ]
###############################################################################
```

#### Exploring and preparing the data

Let's use tm() and stringr() package here (among many others).

##### Steps:

0. For the following process, I'd like to crate separate corpuses for tweets/user description and for training/test set data
1. Transform textual data into bag-of-words: Create a corpus - Corpus()
2. Mapping (transforming) the corpus: Clean texts - tm_map()
3. Tokenization
4. Making a sparse matrix


##### Cleaning text

1. White spaces: including tabs, spaces, line changes, ...
2. Change characters into lower case
3. Correcting misspells
4. Stemming: this can be related with 3.
5. Weird coding errors: there could be errors in unicode and some language-specific coding issues. e.g., &lt; &gt; &amp; \xd0 \xd4. Some weird characters too.
6. Non-English language: for the purpose of this project, we only focuses on English language
7. Links: we can analyze link later, but it’s good to know how to strip link information
8. Punctuations
9. Stopwords: I think it’s really important customize the list of stop words in Twitter context.
10. Mentions and RTs: We may want to isolate @mentions and RT @retweets too.

More about regular expression: http://www.seasite.niu.edu/trans/Regular_Expression_Tutorial.htm

```{r}
###############################################################################
# 1. Use stringr() to remove non-alphanumeric words before creating a corpus
library(stringr); library(dplyr); library(tm)

# Create cleanCorpus function for this
## Note: Use a variable from dataframe instead of using a corpus object

cleanCorpus <- function(corpus) {
 corpus.tmp <- corpus %>%
  str_replace_all("[^[:graph:]]", " ") %>% #remove graphical chars
  str_replace_all("([@][0-9A-Za-z_:]+)", "") %>% #remove @username
  str_replace_all("RT |MT ", "") %>% #remove RT and MT
  str_replace_all("([#][0-9A-Za-z_:]+)", "") %>% #remove #hashtag
  str_replace_all("'s", "") %>% #remove 's
  str_replace_all("[:;\\/.,_'|\"()!?%-]", "") %>% #remove punctuations except #
  str_replace_all("[[:space:]]", " ") %>% #remove white space
  str_replace_all("(http[0-9A-Za-z]+)", "") %>% #remove links
  tolower(.) # change into lowercase
 return(corpus.tmp)
}

# Unused line
#str_replace_all("[^[:alnum:]]|â|Â|???|ð|ã|î", "") #remove graphical chars
###############################################################################
```

#### Current problems and suggestions when mapping texts

- "/" could be changed into "(space)" instead of removing it
- Customize Stopwords
- Stemming
- Customize stem words
- RT information can be extracted and used to train and if we want to do it, we should extract that information before construct corpus and create a variable in a dataframe
- Mention information can be extracted easily by using "in_reply_to_screen_name"


#### After cleaned text, create separate corpus objects

```{r}
# 2. Clean text using cleanCorpus()

training_tweet_clean <- cleanCorpus(training$text)
training_descrip_clean <- cleanCorpus(training$description)
testing_tweet_clean <- cleanCorpus(testing$text)
testing_descrip_clean <- cleanCorpus(testing$description)


# 3. Creating a corpus
training_tweet_corpus <- Corpus(VectorSource(training_tweet_clean))
training_descrip_corpus <- Corpus(VectorSource(training_descrip_clean))
testing_tweet_corpus <- Corpus(VectorSource(testing_tweet_clean))
testing_descrip_corpus <- Corpus(VectorSource(testing_descrip_clean))

# Use inspect() to look at the contents of the corpus
inspect(training_descrip_corpus[1:3])

###############################################################################
```