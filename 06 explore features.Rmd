---
title: "06 examine features"
author: "JungHwan Yang"
date: "March 17, 2015"
output: html_document
---

```{r read data, eval = F}
###############################################################################
path <- "../R Project//Data/FinalHandcoded1300.csv"
d <- read.csv (path)
###############################################################################
```

#### Divide data into training and test set using caret() package

```{r create training/test set}
###############################################################################
library(caret)
inTrain <- createDataPartition(y = d$User_Ideology, p = .75, list = F)
training <- d[inTrain, ]
testing <- d[-inTrain, ]
###############################################################################
```

#### Exploring and preparing the data

Let's use tm() package here (among many others).

Steps:

0. For the following process, I'd like to crate separate corpuses for tweets/user description and for training/test set data
1. Transform textual data into bag-of-words: Create a corpus - Corpus()
2. Mapping (transforming) the corpus: Clean texts - tm_map()
3. Tokenization
4. Making a sparse matrix


##### Cleaning text

(1) White spaces: including tabs, spaces, line changes, ...
(2) Change characters into lower case
(3) Correcting misspells
(4) Stemming: this can be related with (3)
(5) Weird coding errors: there could be errors in unicode and some language-specific coding issues. (e.g., &lt; &gt; &amp; \xd0 \xd4) Some weird characters too.
(6) Non-English language: for the purpose of this project, we only focuses on English language
(7) Links: we can analyze link later, but it’s good to know how to strip link information
(8) Punctuations
(9) Stopwords: I think it’s really important customize the list of stop words in Twitter context.
(10) Mentions and RTs: We may want to isolate @mentions and RT @retweets too.

http://www.seasite.niu.edu/trans/Regular_Expression_Tutorial.htm

```{r}
###############################################################################
# 1. Use stringr() to remove non-alphanumeric words before creating a corpus
library(stringr); library(dplyr); library(tm)

# Create cleanCorpus function for this
## Note: Use a variable from dataframe instead of using a corpus object

cleanCorpus <- function(corpus) {
 corpus.tmp <- corpus %>%
  str_replace_all("[^[:graph:]]", " ") %>% #remove graphical chars
  str_replace_all("([@][0-9A-Za-z_:]+)", "") %>% #remove @username
  str_replace_all("RT |MT ", "") %>% #remove RT and MT
  str_replace_all("([#][0-9A-Za-z_:]+)", "") %>% #remove #hashtag
  str_replace_all("'s", "") %>% #remove 's
  str_replace_all("[:;\\/.,_'|\"()!?%-]", "") %>% #remove punctuations except #
  str_replace_all("[[:space:]]", " ") %>% #remove white space
  str_replace_all("(http[0-9A-Za-z]+)", "") %>% #remove links
  tolower(.) # change into lowercase
 return(corpus.tmp)
}

# Unused line
#str_replace_all("[^[:alnum:]]|â|Â|???|ð|ã|î", "") #remove graphical chars
###############################################################################
```

#### Several suggestions when mapping texts

- "/" could be changed into "(space)" instead of removing it
- Customize Stopwords
- Stemming
- Customize stem words
- RT information can be extracted and used to train and if we want to do it, we should extract that information before construct corpus and create a variable in a dataframe
- Mention information can be extracted easily by using "in_reply_to_screen_name"

```{r}
# 2. Creating a corpus
training_tweet_corpus <- Corpus(VectorSource(training$text))
training_descrip_corpus <- Corpus(VectorSource(training$description))
testing_tweet_corpus <- Corpus(VectorSource(testing$text))
testing_descrip_corpus <- Corpus(VectorSource(testing$description))

# Use inspect() to look at the contents of the corpus
inspect(training_descrip_corpus[1:3])


# 3. Mapping: Clean the corpus
## Use pipelines to make the codes simple (magrittr())
library(magrittr)


training_tweet_corpus_clean <- tm_map(training_tweet_corpus, removeNumbers)
training_tweet_corpus_clean <- tm_map(training_tweet_corpus_clean, tolower)

training_descrip_corpus_clean <- 
testing_tweet_corpus_clean <- tm_map(training_tweet_corpus, tolower)
testing_descrip_corpus_clean <- Corpus(VectorSource(testing$description))


###############################################################################
```

```{r}
cleanCorpus <- function(corpus) {
 corpus.tmp <- tm_map(corpus, removePunctuation)
 corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
 corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
 corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
 return(corpus.tmp)
}

shopping_list <- c("apples x4", "flour", "sugar", "milk x2")
str_extract(shopping_list, "\\d")
str_extract(shopping_list, "[a-z]+")
str_extract(shopping_list, "[a-z]{1,4}")
str_extract(shopping_list, "\\b[a-z]{1,4}\\b")

