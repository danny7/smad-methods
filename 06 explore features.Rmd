---
title: "06 examine features"
author: "JungHwan Yang"
date: "March 17, 2015"
output: html_document
---

#### Read the humancoded data

```{r read data, eval = F}
###############################################################################
path <- "../R Project//Data/FinalHandcoded1300.csv"
d <- read.csv (path)
###############################################################################
```

#### Divide data into training and test set using caret() package

```{r create training/test set}
###############################################################################
library(caret)
inTrain <- createDataPartition(y = d$User_Ideology, p = .75, list = F)
training <- d[inTrain, ]
testing <- d[-inTrain, ]
###############################################################################
```

#### Exploring and preparing the data

Let's use tm() and stringr() package here (among many others).

##### Steps:

0. For the following process, I'd like to crate separate corpuses for tweets/user description and for training/test set data
1. Transform textual data into bag-of-words: Create a corpus - Corpus()
2. Mapping (transforming) the corpus: Clean texts - tm_map()
3. Tokenization
4. Making a sparse matrix


##### Cleaning text

1. White spaces: including tabs, spaces, line changes, ...
2. Change characters into lower case
3. Correcting misspells
4. Stemming: this can be related with 3.
5. Weird coding errors: there could be errors in unicode and some language-specific coding issues. e.g., &lt; &gt; &amp; \xd0 \xd4. Some weird characters too.
6. Non-English language: for the purpose of this project, we only focuses on English language
7. Links: we can analyze link later, but it’s good to know how to strip link information
8. Punctuations
9. Stopwords: I think it’s really important customize the list of stop words in Twitter context.
10. Mentions and RTs: We may want to isolate @mentions and RT @retweets too.

More about regular expression: http://www.seasite.niu.edu/trans/Regular_Expression_Tutorial.htm

```{r}
###############################################################################
# 1. Use stringr() to remove non-alphanumeric words before creating a corpus
library(stringr); library(dplyr); library(tm)

# Create cleanCorpus function for this
## Note: Use a variable from dataframe instead of using a corpus object

cleanCorpus <- function(corpus) {
 corpus.tmp <- corpus %>%
  str_replace_all("[^[:graph:]]", " ") %>% #remove graphical chars
  str_replace_all("([@][0-9A-Za-z_:]+)", "") %>% #remove @username
  str_replace_all("RT |MT ", "") %>% #remove RT and MT
  str_replace_all("([#][0-9A-Za-z_:]+)", "") %>% #remove #hashtag
  str_replace_all("'s", "") %>% #remove 's
  str_replace_all("[:;\\/.,_'|\"()!?%-]", "") %>% #remove punctuations except #
  str_replace_all("[[:space:]]", " ") %>% #remove white space
  str_replace_all("(http[0-9A-Za-z]+)", "") %>% #remove links
  tolower(.) # change into lowercase
 return(corpus.tmp)
}

# Unused line
#str_replace_all("[^[:alnum:]]|â|Â|???|ð|ã|î", "") #remove graphical chars
###############################################################################
```

#### Current problems and suggestions when mapping texts

- "/" could be changed into "(space)" instead of removing it
- Customize Stopwords
- Stemming
- Customize stem words
- RT information can be extracted and used to train and if we want to do it, we should extract that information before construct corpus and create a variable in a dataframe
- Mention information can be extracted easily by using "in_reply_to_screen_name"


#### After cleaned text, create separate corpus objects

```{r}
###############################################################################
# 2. Clean text of training set using cleanCorpus()
## Treat R, L, U separately
training_tweet_clean_C <-
  cleanCorpus(training$text[training$User_Ideology=="C"])
training_tweet_clean_L <-
  cleanCorpus(training$text[training$User_Ideology=="L"])
training_tweet_clean_U <-
  cleanCorpus(training$text[training$User_Ideology=="U"])
training_desc_clean_C <- 
  cleanCorpus(training$description[training$User_Ideology=="C"])
training_desc_clean_L <- 
  cleanCorpus(training$description[training$User_Ideology=="L"])
training_desc_clean_U <- 
  cleanCorpus(training$description[training$User_Ideology=="U"])


# 3. Creating a corpus
training_tweet_corpus_C <-
  Corpus(VectorSource(training_tweet_clean_C))
training_tweet_corpus_L <-
  Corpus(VectorSource(training_tweet_clean_L))
training_tweet_corpus_U <-
  Corpus(VectorSource(training_tweet_clean_U))
training_desc_corpus_C <- 
  Corpus(VectorSource(training_desc_clean_C))
training_desc_corpus_L <- 
  Corpus(VectorSource(training_desc_clean_L))
training_desc_corpus_U <- 
  Corpus(VectorSource(training_desc_clean_U))

# Use inspect() to look at the contents of the corpus
inspect(training_desc_corpus_C[1:10])

# 4. Create a dictionary (using tweet and user description in training set)
tweet_dict <-
  cleanCorpus(cbind(as.character(training$text),
                    as.character(training$description))) %>%
  VectorSource(.) %>%
  Corpus(.) %>%
  DocumentTermMatrix(.) %>%
  findFreqTerms(., 5) %>%
  c(.)

# 5. Create sparse matrices
training_tweet_dtm_C <-
  DocumentTermMatrix(training_tweet_corpus_C,
                     list(dictionary = tweet_dict))
training_tweet_dtm_L <-
  DocumentTermMatrix(training_tweet_corpus_L,
                     list(dictionary = tweet_dict))
training_tweet_dtm_U <-
  DocumentTermMatrix(training_tweet_corpus_U,
                     list(dictionary = tweet_dict))
training_desc_dtm_C <-
  DocumentTermMatrix(training_desc_corpus_C,
                     list(dictionary = tweet_dict))
training_desc_dtm_L <-
  DocumentTermMatrix(training_desc_corpus_L,
                     list(dictionary = tweet_dict))
training_desc_dtm_U <-
  DocumentTermMatrix(training_desc_corpus_U,
                     list(dictionary = tweet_dict))

inspect(training_desc_dtm_C)
findFreqTerms(training_tweet_dtm_C, 10)
findFreqTerms(training_tweet_dtm_L, 10)
findFreqTerms(training_tweet_dtm_U, 10)
findFreqTerms(training_desc_dtm_C, 10)
findFreqTerms(training_desc_dtm_L, 10)
findFreqTerms(training_desc_dtm_U, 10)
###############################################################################
```

### Bi-gram analysis
http://stackoverflow.com/questions/23655694/r-find-most-frequent-group-of-words-in-corpus